{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNVgIdlnSYtj",
        "outputId": "6b9be463-2117-449b-d472-2ae0308aad78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=00d2d83e2bf08fb1c3f827896231cbb7e1018c4a3216271eb8e65607a51d9b37\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5qyBPlS1T70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b819f7-ca7f-4e24-ff54-60a83ea8d7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8821346461415441\n",
            "Feature Importances:  (24,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.06549499439107452,0.0101180157535006,0.031199235699997895,0.012497497128369688,0.04307187692174649,0.006078896195913981,0.012785448510790266,0.03561998592277536,0.0713194377953795,0.022929300482456717,0.011365409294024787,0.006291488260067146,0.021727482276040256,0.014688205254894204,0.011125927276829284,0.015032941782811029,0.017099020779312656,0.19399416177132295,0.018710725931583738,0.036841322110703414,0.06778868554253312,0.040669995192406584,0.1335228519948099,0.10002709373065585])\n",
            "+-----------+-----------+----------+\n",
            "|Customer ID|Churn Label|prediction|\n",
            "+-----------+-----------+----------+\n",
            "| 0004-TLHLJ|        Yes|       1.0|\n",
            "| 0013-SMEOE|         No|       0.0|\n",
            "| 0015-UOCOJ|         No|       0.0|\n",
            "| 0019-EFAEP|         No|       0.0|\n",
            "| 0023-HGHWL|        Yes|       1.0|\n",
            "| 0030-FNXPP|         No|       0.0|\n",
            "| 0042-RLHYP|         No|       0.0|\n",
            "| 0057-QBUQH|         No|       0.0|\n",
            "| 0078-XZMHT|         No|       0.0|\n",
            "| 0080-EMYVY|         No|       0.0|\n",
            "| 0080-OROZO|         No|       0.0|\n",
            "| 0082-OQIQY|         No|       0.0|\n",
            "| 0089-IIQKO|         No|       0.0|\n",
            "| 0096-BXERS|         No|       0.0|\n",
            "| 0106-UGRDO|         No|       0.0|\n",
            "| 0114-PEGZZ|         No|       0.0|\n",
            "| 0125-LZQXK|        Yes|       1.0|\n",
            "| 0178-CIIKR|         No|       0.0|\n",
            "| 0196-VULGZ|        Yes|       1.0|\n",
            "| 0203-HHYIJ|        Yes|       0.0|\n",
            "+-----------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Customer Churn Prediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/BDA-Project.csv\", header=True, inferSchema=True)\n",
        "df = df.drop(\"Satisfaction Score\", \"Churn Category\", \"Churn Reason\")\n",
        "categorical_columns = [\"City\", \"Gender\", \"Senior Citizen\", \"Married\", \"Dependents\", \"Phone Service\",\n",
        "                       \"Multiple Lines\", \"Internet Service\", \"Internet Type\", \"Online Security\",\n",
        "                       \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
        "                       \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\", \"Unlimited Data\",\n",
        "                       \"Contract\", \"Paperless Billing\", \"Payment Method\"]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in categorical_columns]\n",
        "assembler = VectorAssembler(inputCols=[column+\"_index\" for column in categorical_columns] +\n",
        "                            [\"Age\", \"Number of Dependents\", \"Tenure in Months\", \"Total Revenue\"],\n",
        "                            outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "label_indexer = StringIndexer(inputCol=\"Churn Label\", outputCol=\"label\")\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler, label_indexer])\n",
        "\n",
        "prepared_df = pipeline.fit(df).transform(df)\n",
        "train_df, test_df = prepared_df.randomSplit([0.8, 0.2], seed=42)\n",
        "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", seed=42)\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).addGrid(rf.maxDepth, [5, 10]).build()\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "cv_model = cv.fit(train_df)\n",
        "\n",
        "predictions_test = cv_model.transform(test_df)\n",
        "accuracy = evaluator.evaluate(predictions_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "rf_model = cv_model.bestModel\n",
        "print(\"Feature Importances: \", rf_model.featureImportances)\n",
        "\n",
        "predictions_test.select(\"Customer ID\", \"Churn Label\", \"prediction\").show()"
      ]
    }
  ]
}